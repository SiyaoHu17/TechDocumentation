pip: !pip install -U imbalanced-learn


Resources:
https://www.dataquest.io/blog/machine-learning-preparing-data/
https://amueller.github.io/COMS4995-s18/slides/aml-08-021218-imputation-feature-selection/#32


For most "average" problems, you should have 10,000 - 100,000 examples. 
For “hard” problems like machine translation, high dimensional data generation, or anything requiring deep learning, you should try to get 100,000 - 1,000,000 examples. 
Generally, the more dimensions your data has, the more data you need.


https://machinelearningmastery.com/an-introduction-to-feature-selection/

label encoding
https://docs.w3cub.com/scikit_learn/modules/generated/sklearn.preprocessing.onehotencoder/
https://stackoverflow.com/questions/54570947/feature-names-from-onehotencoder
https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html
https://stackoverflow.com/questions/56338847/how-to-give-column-names-after-one-hot-encoding-with-sklearn
https://medium.com/@contactsunny/label-encoder-vs-one-hot-encoder-in-machine-learning-3fc273365621

missing value
Theoretically, 25 to 30% is the maximum missing values are allowed, beyond which we might want to drop the variable from analysis. Practically this varies.At 278 times we get variables with ~50% of missing values but still the customer insist to have it for analyzing. In those cases we might want to treat them accordingly.

imbalanced data
https://stats.stackexchange.com/questions/262616/roc-vs-precision-recall-curves-on-imbalanced-dataset





Tunning for decision tree
https://medium.com/@mohtedibf/indepth-parameter-tuning-for-decision-tree-6753118a03c3
https://machinelearningmastery.com/tune-number-size-decision-trees-xgboost-python/
https://towardsdatascience.com/fine-tuning-xgboost-in-python-like-a-boss-b4543ed8b1e
https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/
https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/




cross validation
https://scikit-learn.org/stable/modules/cross_validation.html
https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html
https://stackoverflow.com/questions/29438265/stratified-train-test-split-in-scikit-learn
https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html
with upsampling
https://medium.com/lumiata/cross-validation-for-imbalanced-datasets-9d203ba47e8
https://github.com/lumiata/tech_blog/blob/master/Cross_Validation_Imbalanced_Datasets/cross-validation.ipynb
with feature selection
http://thatdatatho.com/2018/10/04/cross-validation-the-wrong-way-right-way-feature-selection/



feature importance
https://stackoverflow.com/questions/51905524/plot-importance-variables-xgboost-python
https://github.com/lumiata/tech_blog/blob/master/Cross_Validation_Imbalanced_Datasets/cross-validation.ipynb


visualize decision tree
https://towardsdatascience.com/visualizing-decision-trees-with-python-scikit-learn-graphviz-matplotlib-1c50b4aa68dc
